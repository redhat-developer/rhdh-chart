name: Test Charts

on:
  pull_request:
    branches:
      - main
      - rhdh-1.[0-9]+
      - 1.[0-9]+.x
      - release-1.[0-9]+
    paths:
      - "charts/**"
      - ".github/**"
      - ".pre-commit/**"
      - "ct-install.yaml"
      - "ct.yaml"

concurrency:
  group: ${{ github.workflow }}-${{ github.event.number }}
  cancel-in-progress: true

jobs:
  check-metadata:
    name: Lint Metadata
    runs-on: ubuntu-latest
    env:
      GO111MODULE: on
    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4

      - uses: actions/setup-python@42375524e23c412d93fb67b49958b491fce71c38 # v5
        with:
          python-version: 3.13

      - uses: actions/setup-go@f111f3307d8850f501ac008e886eec1fd1932a34 # v5
        with:
          go-version: ^1

      - name: Setup helm-docs
        run: go install github.com/norwoodj/helm-docs/cmd/helm-docs@latest

      - name: Run pre-commit
        uses: pre-commit/action@2c7b3805fd2a0fd8c1884dcaebf91fc102a13ecd # v3.0.1
        with:
          extra_args: --show-diff-on-failure

  test-chart:
    strategy:
      matrix:
        version:
          # 'latest' is the latest RC or stable release of RHDH (built from a release branch)
          - latest
        experimental: [false]
        include:
          # 'next' is the upcoming release of RHDH
          # (built from the RHDH main branch, so might be unstable)
          - version: next
            experimental: true

    # Aligning job name with the OpenShift CI config: https://github.com/openshift/release/blob/master/core-services/prow/02_config/redhat-developer/rhdh-chart/_prowconfig.yaml#L18
    name: Test ${{ matrix.version == 'latest' && 'Latest' || matrix.version }} Release
    runs-on: ubuntu-latest
    continue-on-error: ${{ matrix.experimental }}

    steps:
      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4
        with:
          fetch-depth: 0

      - name: Set up Helm
        uses: azure/setup-helm@5119fcb9089d432beecbf79bb2c7915207344b78 # renovate: tag=v3.5
        with:
          version: v3.10.0

      - uses: actions/setup-python@42375524e23c412d93fb67b49958b491fce71c38 # v5
        with:
          python-version: 3.13

      - name: Set up chart-testing
        uses: helm/chart-testing-action@0d28d3144d3a25ea2cc349d6e59901c4ff469b3b # v2.7.0

      - name: "Add NGINX Ingress and Bitnami Repository"
        run: |
          helm repo add ingress-nginx "https://kubernetes.github.io/ingress-nginx"
          helm repo add bitnami "https://charts.bitnami.com/bitnami"
          helm repo add backstage https://backstage.github.io/charts
          helm repo update

      - name: Lint chart
        run: |
          ct lint \
            --debug \
            --config ct.yaml \
            --target-branch "${{ github.event.pull_request.base.ref }}" \
            --helm-extra-args="--set upstream.backstage.image.tag=${{ matrix.version }} --set upstream.ingress.enabled=true"

      - name: Generate KinD Config
        run: |
          cat <<EOF > /tmp/kind-config.yaml
          apiVersion: kind.x-k8s.io/v1alpha4
          kind: Cluster
          nodes:
            - role: control-plane
              extraPortMappings:
                - containerPort: 80
                  hostPort: 80
                  protocol: TCP
                - containerPort: 443
                  hostPort: 443
                  protocol: TCP
          EOF

      - name: Create KIND Cluster
        uses: helm/kind-action@a1b0e391336a6ee6713a0583f8c6240d70863de3 # v1.12.0
        with:
          config: /tmp/kind-config.yaml

      - name: Create custom storage class
        run: |
          export defaultScProvisioner=$(kubectl get storageclass -o jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubernetes\.io/is-default-class=="true")].provisioner}')
          if [[ -z "$defaultScProvisioner" ]]; then
            echo "No default storage class found or it has no provisioner. Exiting early because the test using the custom Storage Class will likely fail. Use a cluster that has a default storage class."
            exit 1
          fi
          echo "[INFO] defaultScProvisioner=$defaultScProvisioner"

          cat <<EOF | kubectl apply -f -
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: custom-sc
          # same provisioner as the one used by the default storage class on the cluster
          provisioner: $defaultScProvisioner
          reclaimPolicy: Delete
          volumeBindingMode: WaitForFirstConsumer
          EOF

          kubectl get storageclass custom-sc -o yaml

      - name: Install Ingress Controller
        run: |
          helm install ingress-nginx/ingress-nginx --generate-name \
            --set controller.service.type='NodePort' \
            --set controller.admissionWebhooks.enabled=false

      - name: Install Operator Lifecycle Manager (OLM)
        # In case we need to install additional Operators
        env:
          OLM_VERSION: "v0.31.0"
        run: |
          curl -L "https://github.com/operator-framework/operator-lifecycle-manager/releases/download/${OLM_VERSION}/install.sh" -o install-olm.sh
          chmod +x install-olm.sh
          ./install-olm.sh "${OLM_VERSION}"

      - name: Run chart-testing
        run: |
          ct install \
            --debug \
            --config ct-install.yaml \
            --upgrade \
            --target-branch "${{ github.event.pull_request.base.ref }}" \
            --helm-extra-set-args="--set upstream.backstage.image.tag=${{ matrix.version }} --set upstream.ingress.enabled=true --set global.host=rhdh.127.0.0.1.sslip.io"
